{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJDML7TQDRUI"
      },
      "source": [
        "## Activity 4: Data Preparation and Cleaning for Credit card fraud detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrkNQje1DRUM"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set plotting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYPm6b4BDRUO"
      },
      "outputs": [],
      "source": [
        "#Load the datasets 'fraudTrain.csv' and 'fraudTest.csv'\n",
        "train_df = pd.read_csv(\"fraudTrain.csv\")\n",
        "test_df = pd.read_csv(\"fraudTest.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRyxga-NDRUP"
      },
      "outputs": [],
      "source": [
        "#Explore the data entries and shape of the datasets\n",
        "print(\"Train dataset shape:\", train_df.shape)\n",
        "print(\"Test dataset shape:\", test_df.shape)\n",
        "\n",
        "print(\"\\nTrain data preview:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTest data preview:\")\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgi_H6LiDRUR"
      },
      "outputs": [],
      "source": [
        "#Check the correlation between numerical features and plot heatmap in both datasets\n",
        "train_corr = train_df.select_dtypes(include=np.number).corr()\n",
        "test_corr = test_df.select_dtypes(include=np.number).corr()\n",
        "\n",
        "# Heatmap for train dataset\n",
        "plt.title(\"Correlation Heatmap - Train\")\n",
        "sns.heatmap(train_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.show()\n",
        "\n",
        "# Heatmap for test dataset\n",
        "plt.title(\"Correlation Heatmap - Test\")\n",
        "sns.heatmap(test_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGPVjBHcDRUS"
      },
      "outputs": [],
      "source": [
        "#Drop unnecessary columns like 'Unnamed: 0'\n",
        "train_df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
        "test_df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqsExj5ZDRUS"
      },
      "outputs": [],
      "source": [
        "#Handle missing values and duplicate entries\n",
        "# Check missing values\n",
        "print(\"\\nMissing values in train dataset:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in test dataset:\")\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "# Drop duplicate entries\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "test_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIDkGnBXDRUT"
      },
      "outputs": [],
      "source": [
        "#Convert dob,trans_date_trans_time column in both test & train to datetime data type and creating new 'trans_date' column\n",
        "for df in [train_df, test_df]:\n",
        "    df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
        "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
        "    df['trans_date'] = df['trans_date_trans_time'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr-Ia3UYDRUT"
      },
      "outputs": [],
      "source": [
        "#Check the Fraud and the Normal transaction numbers for test and train datasets\n",
        "print(\"\\nTrain dataset fraud vs normal:\")\n",
        "print(train_df['is_fraud'].value_counts())\n",
        "\n",
        "print(\"\\nTest dataset fraud vs normal:\")\n",
        "print(test_df['is_fraud'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTHpWL6DDRUU"
      },
      "outputs": [],
      "source": [
        "#Merge train and test datasets for exploratory data analysis\n",
        "merged_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
        "print(\"\\nMerged dataset shape:\", merged_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPUJ1IpEDRUU"
      },
      "source": [
        "## Activity 5: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-cUK-7EDRUV"
      },
      "outputs": [],
      "source": [
        "#Count plot of Transactions by top 10 Categories in merged datasets and provide the conclusions and insights gathered\n",
        "top_categories = merged_df['category'].value_counts().nlargest(10)\n",
        "\n",
        "sns.countplot(data=merged_df[merged_df['category'].isin(top_categories.index)],\n",
        "              y='category', order=top_categories.index, palette='viridis')\n",
        "plt.title(\"Top 10 Categories by Number of Transactions\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg2O-D_dDRUV"
      },
      "outputs": [],
      "source": [
        "#Count plot of Transactions by Gender in merged datasets and provide the conclusions and insights gathered\n",
        "sns.countplot(data=merged_df, x='gender', palette='Set2')\n",
        "plt.title(\"Transactions by Gender\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjchdFrPDRUW"
      },
      "outputs": [],
      "source": [
        "#Count plot of Transactions by top 10 Merchants in merged datasets and provide the conclusions and insights gathered\n",
        "top_merchants = merged_df['merchant'].value_counts().nlargest(10)\n",
        "\n",
        "sns.countplot(data=merged_df[merged_df['merchant'].isin(top_merchants.index)],\n",
        "              y='merchant', order=top_merchants.index, palette='mako')\n",
        "plt.title(\"Top 10 Merchants by Number of Transactions\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Merchant\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpEpdfhkDRUW"
      },
      "outputs": [],
      "source": [
        "#Box plot of transaction amount by category and provide the conclusions and insights gathered\n",
        "sns.boxplot(data=merged_df, x='category', y='amt')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Transaction Amount by Category\")\n",
        "plt.ylabel(\"Transaction Amount ($)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrusL526DRUX"
      },
      "outputs": [],
      "source": [
        "#A histogram of the transaction amount and provide the conclusions and insights gathered\n",
        "sns.histplot(merged_df['amt'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Transaction Amounts\")\n",
        "plt.xlabel(\"Amount ($)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8lUTIj0DRUY"
      },
      "outputs": [],
      "source": [
        "#Top Categories of the Fraudulent Transactions and provide the conclusions and insights gathered\n",
        "fraud_df = merged_df[merged_df['is_fraud'] == 1]\n",
        "top_fraud_categories = fraud_df['category'].value_counts().nlargest(10)\n",
        "\n",
        "sns.countplot(data=fraud_df[fraud_df['category'].isin(top_fraud_categories.index)],\n",
        "              y='category', order=top_fraud_categories.index, palette='flare')\n",
        "plt.title(\"Top Fraud Categories\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdFmQXd0DRUY"
      },
      "outputs": [],
      "source": [
        "#State-wise Analysis of Fraudulent Transactions and provide the conclusions and insights gathered\n",
        "fraud_by_state = fraud_df['state'].value_counts().nlargest(10)\n",
        "\n",
        "sns.barplot(x=fraud_by_state.values, y=fraud_by_state.index, palette='magma')\n",
        "plt.title(\"Top 10 States with Fraudulent Transactions\")\n",
        "plt.xlabel(\"Number of Fraudulent Transactions\")\n",
        "plt.ylabel(\"State\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N4aHWH3DRUZ"
      },
      "outputs": [],
      "source": [
        "#City-wise Analysis of Fraudulent Transactions and provide the conclusions and insights gathered\n",
        "fraud_by_city = fraud_df['city'].value_counts().nlargest(10)\n",
        "\n",
        "sns.barplot(x=fraud_by_city.values, y=fraud_by_city.index, palette='rocket')\n",
        "plt.title(\"Top 10 Cities with Fraudulent Transactions\")\n",
        "plt.xlabel(\"Number of Fraudulent Transactions\")\n",
        "plt.ylabel(\"City\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtznIdZuDRUZ"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by Job and provide the conclusions and insights gathered\n",
        "fraud_by_job = fraud_df['job'].value_counts().nlargest(10)\n",
        "\n",
        "sns.barplot(x=fraud_by_job.values, y=fraud_by_job.index, palette='cool')\n",
        "plt.title(\"Top 10 Jobs with Fraudulent Transactions\")\n",
        "plt.xlabel(\"Number of Frauds\")\n",
        "plt.ylabel(\"Job\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeRidQWoDRUa"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by Age Groups and provide the conclusions and insights gathered\n",
        "# Hitung usia\n",
        "fraud_df['age'] = (pd.to_datetime('2020-01-01') - fraud_df['dob']).dt.days // 365\n",
        "\n",
        "# Buat kelompok usia\n",
        "fraud_df['age_group'] = pd.cut(fraud_df['age'], bins=[18, 25, 35, 45, 55, 65, 100],\n",
        "                               labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
        "\n",
        "sns.countplot(data=fraud_df, x='age_group', palette='crest')\n",
        "plt.title(\"Frauds by Age Group\")\n",
        "plt.xlabel(\"Age Group\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPyOcjzcDRUb"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by Gender and provide the conclusions and insights gathered\n",
        "sns.countplot(data=fraud_df, x='gender', palette='Set3')\n",
        "plt.title(\"Frauds by Gender\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-1Um_b_DRUb"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by Year and provide the conclusions and insights gathered\n",
        "fraud_df['year'] = pd.to_datetime(fraud_df['trans_date_trans_time']).dt.year\n",
        "\n",
        "sns.countplot(data=fraud_df, x='year', palette='YlGnBu')\n",
        "plt.title(\"Frauds by Year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ2PhIlyDRUb"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by latitudinal distance and provide the conclusions and insights gathered\n",
        "sns.histplot(fraud_df['lat'], bins=30, kde=True, color='darkred')\n",
        "plt.title(\"Distribution of Fraud by Latitude\")\n",
        "plt.xlabel(\"Latitude\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv0EbxQjDRUc"
      },
      "outputs": [],
      "source": [
        "#Number of Credit Card Frauds by longitudinal distance and provide the conclusions and insights gathered\n",
        "sns.histplot(fraud_df['long'], bins=30, kde=True, color='darkblue')\n",
        "plt.title(\"Distribution of Fraud by Longitude\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Number of Frauds\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdBuiUF4DRUc"
      },
      "source": [
        "## Activity 6: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DCQ__uXDRUc"
      },
      "outputs": [],
      "source": [
        "#Encode categorical variables using techniques suitable for the model, such as one-hot encoding\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Salin dataframe untuk diproses\n",
        "df = merged_df.copy()\n",
        "\n",
        "# One-hot encode untuk 'category' dan 'gender'\n",
        "df = pd.get_dummies(df, columns=['category', 'gender'], drop_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk0OFtU1DRUd"
      },
      "outputs": [],
      "source": [
        "#Drop highly correlated or redundant features to reduce dimensionality\n",
        "redundant_cols = ['trans_date_trans_time', 'trans_date', 'dob', 'unix_time',\n",
        "                  'cc_num', 'merchant', 'first', 'last', 'street', 'city', 'state', 'trans_num']\n",
        "df.drop(columns=redundant_cols, inplace=True, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So7va83gDRUd"
      },
      "outputs": [],
      "source": [
        "#Label encode categorical features\n",
        "if df['job'].dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    df['job'] = le.fit_transform(df['job'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwjlsTxqDRUd"
      },
      "outputs": [],
      "source": [
        "#Normalize or standardize numerical features to improve model performance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Tambahkan kolom 'age' berdasarkan kolom dob\n",
        "df['age'] = (pd.to_datetime(\"2020-01-01\") - pd.to_datetime(merged_df['dob'])).dt.days // 365\n",
        "\n",
        "# Normalisasi fitur numerik\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = ['amt', 'lat', 'long', 'zip', 'age']\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1L7Pz4-DRUd"
      },
      "outputs": [],
      "source": [
        "#Save the final dataset as 'Capstone_Dataset.csv'\n",
        "df.to_csv(\"Capstone_Dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSpkqxi6DRUe"
      },
      "source": [
        "## Activity 7: Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn65FaCUDRUe"
      },
      "outputs": [],
      "source": [
        "#Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Fitur & label\n",
        "X = df.drop(columns=['is_fraud'])\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Stratified split agar distribusi fraud seimbang\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Oversampling\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIimRDA_DRUe"
      },
      "outputs": [],
      "source": [
        "#Train a logistic regression, Decision Tree, Random Forest, Adaboost, GaussianNB, KNN classifier, and LightGBM Classifier, model to predict fraudulent transactions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# Model dictionary\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
        "    \"Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
        "    \"AdaBoost\": AdaBoostClassifier(),\n",
        "    \"GaussianNB\": GaussianNB(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"LightGBM\": LGBMClassifier()\n",
        "}\n",
        "\n",
        "# Latih semua model\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "    trained_models[name] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25NYNJgyDRUf"
      },
      "outputs": [],
      "source": [
        "#Evaluate the model using metrics such as accuracy, precision, recall, and the F1-score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"\\n Model Evaluation Metrics:\\n\")\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\" {name}\")\n",
        "    print(f\"   - Accuracy : {acc:.4f}\")\n",
        "    print(f\"   - Precision: {prec:.4f}\")\n",
        "    print(f\"   - Recall   : {rec:.4f}\")\n",
        "    print(f\"   - F1-score : {f1:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqMk0v9oDRUf"
      },
      "outputs": [],
      "source": [
        "#Utilize confusion matrix and classification reports to assess performance\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "print(\"\\n Confusion Matrices & Classification Reports:\\n\")\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, digits=4, zero_division=0)\n",
        "\n",
        "    print(f\" {name}\")\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "    print(\"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}