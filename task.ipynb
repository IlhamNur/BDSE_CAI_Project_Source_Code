{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4: Data Preparation and Cleaning for Credit card fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets 'fraudTrain.csv' and 'fraudTest.csv'\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('fraudTrain.csv')\n",
    "test_df = pd.read_csv('fraudTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the data entries and shape of the datasets\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "print(\"\\nTrain columns:\\n\", train_df.columns)\n",
    "print(\"\\nSample train data:\\n\", train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the correlation between numerical features and plot heatmap in both datasets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation heatmap for train set\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(train_df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap - Train Set\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for test set\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(test_df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap - Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns like 'Unnamed: 0'\n",
    "train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle missing values and duplicate entries\n",
    "# Check missing values\n",
    "print(\"Missing values in train:\\n\", train_df.isnull().sum())\n",
    "print(\"\\nMissing values in test:\\n\", test_df.isnull().sum())\n",
    "\n",
    "# Drop duplicates if any\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "test_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dob,trans_date_trans_time column in both test & train to datetime data type and creating new 'trans_date' column\n",
    "for df in [train_df, test_df]:\n",
    "    df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "    df['trans_date'] = df['trans_date_trans_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the Fraud and the Normal transaction numbers for test and train datasets\n",
    "print(\"Train Set:\")\n",
    "print(train_df['is_fraud'].value_counts())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df['is_fraud'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge train and test datasets for exploratory data analysis\n",
    "combined_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "print(\"Combined dataset shape:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 5: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fraud_df = combined_df[combined_df['is_fraud'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count plot of Transactions by top 10 Categories in merged datasets and provide the conclusions and insights gathered\n",
    "top_categories = combined_df['category'].value_counts().nlargest(10)\n",
    "sns.countplot(data=combined_df[combined_df['category'].isin(top_categories.index)],\n",
    "              x='category', order=top_categories.index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Transaction Categories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count plot of Transactions by Gender in merged datasets and provide the conclusions and insights gathered\n",
    "sns.countplot(data=combined_df, x='gender', hue='is_fraud')\n",
    "plt.title(\"Transaction Count by Gender\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count plot of Transactions by top 10 Merchants in merged datasets and provide the conclusions and insights gathered\n",
    "top_merchants = combined_df['merchant'].value_counts().nlargest(10)\n",
    "sns.countplot(data=combined_df[combined_df['merchant'].isin(top_merchants.index)],\n",
    "              x='merchant', order=top_merchants.index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Merchants by Transactions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot of transaction amount by category and provide the conclusions and insights gathered\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=combined_df[combined_df['category'].isin(top_categories.index)],\n",
    "            x='category', y='amt')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Transaction Amount Distribution by Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A histogram of the transaction amount and provide the conclusions and insights gathered\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(combined_df['amt'], bins=50, kde=True)\n",
    "plt.title(\"Histogram of Transaction Amounts\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Categories of the Fraudulent Transactions and provide the conclusions and insights gathered\n",
    "fraud_by_category = fraud_df['category'].value_counts().nlargest(10)\n",
    "sns.barplot(x=fraud_by_category.index, y=fraud_by_category.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top Categories with Fraudulent Transactions\")\n",
    "plt.ylabel(\"Fraud Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#State-wise Analysis of Fraudulent Transactions and provide the conclusions and insights gathered\n",
    "fraud_by_state = fraud_df['state'].value_counts().nlargest(10)\n",
    "sns.barplot(x=fraud_by_state.index, y=fraud_by_state.values)\n",
    "plt.title(\"Top 10 States with Fraudulent Transactions\")\n",
    "plt.ylabel(\"Fraud Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#City-wise Analysis of Fraudulent Transactions and provide the conclusions and insights gathered\n",
    "fraud_by_city = fraud_df['city'].value_counts().nlargest(10)\n",
    "sns.barplot(x=fraud_by_city.index, y=fraud_by_city.values)\n",
    "plt.title(\"Top 10 Cities with Fraudulent Transactions\")\n",
    "plt.ylabel(\"Fraud Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by Job and provide the conclusions and insights gathered\n",
    "fraud_by_job = fraud_df['job'].value_counts().nlargest(10)\n",
    "sns.barplot(x=fraud_by_job.index, y=fraud_by_job.values)\n",
    "plt.title(\"Top Jobs with Fraudulent Transactions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Fraud Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by Age Groups and provide the conclusions and insights gathered\n",
    "combined_df['age'] = 2020 - combined_df['dob'].dt.year\n",
    "combined_df['age_group'] = pd.cut(combined_df['age'], bins=[18, 30, 40, 50, 60, 80], labels=[\"18-30\", \"31-40\", \"41-50\", \"51-60\", \"61-80\"])\n",
    "fraud_age_group = combined_df[combined_df['is_fraud'] == 1]['age_group'].value_counts()\n",
    "\n",
    "sns.barplot(x=fraud_age_group.index, y=fraud_age_group.values)\n",
    "plt.title(\"Fraud Count by Age Group\")\n",
    "plt.ylabel(\"Fraud Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by Gender and provide the conclusions and insights gathered\n",
    "sns.countplot(data=fraud_df, x='gender')\n",
    "plt.title(\"Fraud Count by Gender\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by Year and provide the conclusions and insights gathered\n",
    "fraud_df['year'] = pd.to_datetime(fraud_df['trans_date_trans_time']).dt.year\n",
    "sns.countplot(data=fraud_df, x='year')\n",
    "plt.title(\"Fraud Count by Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by latitudinal distance and provide the conclusions and insights gathered\n",
    "sns.histplot(fraud_df['lat'], bins=30, kde=True)\n",
    "plt.title(\"Fraud Distribution by Latitude\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Credit Card Frauds by longitudinal distance and provide the conclusions and insights gathered\n",
    "sns.histplot(fraud_df['long'], bins=30, kde=True)\n",
    "plt.title(\"Fraud Distribution by Longitude\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 6: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categorical variables using techniques suitable for the model, such as one-hot encoding\n",
    "df_encoded = combined_df.copy()\n",
    "\n",
    "one_hot_features = ['category', 'gender', 'state', 'city', 'merchant']\n",
    "\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=one_hot_features, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop highly correlated or redundant features to reduce dimensionality\n",
    "import numpy as np\n",
    "\n",
    "corr_matrix = df_encoded.select_dtypes(include='number').corr().abs()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "df_encoded.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encode categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_encoded['job'] = le.fit_transform(df_encoded['job'])\n",
    "\n",
    "df_encoded['trans_date'] = pd.to_datetime(df_encoded['trans_date'])\n",
    "df_encoded['year'] = df_encoded['trans_date'].dt.year\n",
    "df_encoded['month'] = df_encoded['trans_date'].dt.month\n",
    "\n",
    "df_encoded.drop(columns=['trans_date', 'trans_date_trans_time', 'dob'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize or standardize numerical features to improve model performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_features = df_encoded.select_dtypes(include=['int64', 'float64']).drop(columns=['is_fraud']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[numeric_features] = scaler.fit_transform(df_encoded[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final dataset as 'Capstone_Dataset.csv'\n",
    "df_encoded.to_csv('Capstone_Dataset.csv', index=False)\n",
    "print(\"Final dataset saved as 'Capstone_Dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 7: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_encoded.drop('is_fraud', axis=1)\n",
    "y = df_encoded['is_fraud']\n",
    "\n",
    "# Split 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a logistic regression, Decision Tree, Random Forest, Adaboost, GaussianNB, KNN classifier, and LightGBM Classifier, model to predict fraudulent transactions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model using metrics such as accuracy, precision, recall, and the F1-score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilize confusion matrix and classification reports to assess performance\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
